### kafka如何保证数据不丢失？
```angular2html
1. producer 到 broker 数据传输
   使用异步传输，传输失败重试，多次失败，可能是硬件问题，需要人工干预。
2. broker 数据
   broker 中分区数据多副本，一个leader有多个follower副本，使用ack确认机制，ack=1表明leader数据同步一个follower后再确认传输成功。
3. broker 到 consumer 数据传输。
   kafka数据在分区中顺序存储，并使用偏移量标记消费的数据下表，默认消费者自动提交偏移量，可以使用手动提交偏移量。
   使用同步+异步提交偏移量，主体逻辑使用异步，在finally中同步提交。
```

### kafka 如何保证数据顺序消费？
```angular2html
1. kafka 数据分区存储，消费者组内的消费者可能会消费多个分区的数据，这样数据无法保证顺序性。可以将需要顺序的消息存储同一个分区，
   这样就能保证消息的顺序性。
```

### kafka的高可用性？
```angular2html
1. 集群，集群中多个broker实例，其中一个broker宕机了，其他broker依旧可以对外提供服务。
2. 分区复制机制，kafka中分区的数据会有多个副本，一个leader多个follower，并存储在多个broker中，如果leader所在的broker
   宕机了，从其中的follower中选一个作为leader。其中follower又分为两类，ISR副本：需要同步备份数据的副本，普通副本：异步备份数据的副本，
   leader宕机了，就从ISR中选取作为leader.
```

### kafka数据清理机制
```angular2html
1. kafka数据存储机制(以文件形式存储)： 数据分区，分区内又分段存储，一个段就是包括(.index索引文件，.log数据文件),
   分段存储可以快速查找数据，避免文件过大，数据查找删除的麻烦。
2. 删除数据有两个策略：依据消息的保留时间，如果消息保留超过指定的时间，自动触发清理。依据topic存储数据的大小，当topic存储
   数据超过阈值，删除最久的数据。默认关闭，需要手动开启。
```

### kafka高性能设计？
```angular2html
1. 零拷贝技术：一般io: 从数据从 磁盘 --> 内核内存 --> java内存 -->内核内存 --> 网卡 经过四次拷贝
             NIO :  使用直接内存，java可以直接访问的内核内存， 数据从 磁盘 --> 内核内存 --> 网卡 经过两次拷贝。
             可以大大提升速度。
2. 顺序读写：分区数据按照顺序存储，磁盘读取数据很快
3. 页缓存：磁盘数据缓存到内存中，对磁盘数据访问变为对内存的访问
4. 消息分区：消息可以存储在不同分区，分区可以在不同集群，可以并发读取数据
```